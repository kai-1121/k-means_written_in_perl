#k-means_written_in_perl

-k-meansを行ったプログラム（k-means.pl）

-100次元に圧縮してk-meansを行ったプログラム（k-means-100ver.pl）


## 前提（データを用意）
このプログラムは以下の作業の後に使うことを前提としている。

1.新聞数年分の社説を確保

2.全文書を形態素解析


## 手順（プログラムの説明）
3．文書ごとに、名刺の出現頻度を求める

4．全文書を使ってidfを作成(全名詞のidf値のリスト)

5．tf*idf値により、文書ごとに重要語句を抽出（上位百語）

6．100語を基底ベクトルとする文章ベクトルを定義

7．すべての社説の文章ベクトル作成

上記手順で、特記事項 については以下。
プログラムではまず、idfを求めるために、各名詞を連想配列の引き値にして回数を数えるとともに、全文章の“。”の数を数えた。
その後、一文章中の各名詞を連想配列の引き値にして回数を数えるとともに、各文章の“。”の数を数えた。
tfの値を一文章中の各名詞の数を一文章中の“。”の数で割ったもの、idfの値を全文章中の各名詞の数を全文章中の“。”の数で割ったものの対数にマイナスをかけたものとして、計算した。
その後、各文章のtf*idf値上位百語を上から順に二元連想配列に取り、一文章を上位百語からなるベクトルとした。

tfを求める際に一文章中の“。”の数で割っているのは、一文章の定義は曖昧で分量が多かったり少なかったりする可能性を考慮している。またidfを求める際に全文章中の“。”の数で割っているのは、全文章数で割ることと意味は変わらない（すべての単語で平等に使うから）。全文章中の各名詞の数を分子として扱っているのは、分子が“文章集合の中で単語が含まれる文章数”だと、文章の分量の差を考慮していなく、それだと文章ごとにある単語の現れる可能性は変わってしまうので、その代わりとして使っている。この場合でも特定の文章に出現する語の数は多くの文章に出現する語の数に比べ、かなり少なくなると考えられるため。役割としては問題ないと思われる。
各文章のtf*idf値上位百語をとるさいに、上位百次元以外の単語は考えなかった（情報として排除した）。上位百語以外の単語も成分ゼロにするなどして、tf*idf値は考えないが単語が存在するという情報は持つということもできたが、計算量が増える等の観点から上記の方法にした。しかし、後に類似度を取る際に削除した情報を使うことになるが、それはプログラムの工夫で解決している。

8．文書間の類似度関数を定義
類似度関数にユークリッド距離を使うといったことも考えられたが、それだと上位百語の単語構成が似ていたとしてもtf*idf値（ベクトルの大きさ）が異なると同じクラスに分類されにくいといったことが起きる可能性がある。そこで類似度にベクトルの長さの効果を反映させず、選出した語の重要度の相対的な分布により類似度が決まるcosine相関度を使った。

9．k-means法によるクラスタリング
最初にクラスをランダム分けるとき、読み込んだ順に番号を振ったファイルをクラスの数で割ったあまりを割り当てるクラス番号とした（例えば101番目に読み込まれたファイルは101％100よりクラス1に割り振られる）。
その後、各クラスタで重心を計算、各文章ベクトルの割り当てを変化がなくなるまで繰り返す。


